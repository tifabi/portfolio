{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #The Impact of Clinical Trial Results on Pharmaceutical Stock Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1) Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Set up Google Alerts by going to: https://www.google.com/alerts\n",
    "        Set alerts to capture news or other articles pertaining to the subject matter\n",
    "        Collected up to date information on clinical trials using key words: \n",
    "            \"Phase III trail\"\n",
    "            \"Phase 3 trial\"\n",
    "            \"Meets Primary End Point\"\n",
    "\n",
    "1.2 Access Google Alert Snippits with Gmail API:\n",
    "    1.2.1 Go to: https://developers.google.com/gmail/api/quickstart/python to configure Authentication\n",
    "    1.2.2 Make sure the client_secret.json is in the same directory\n",
    "    1.2.3 Run Google API parsing script\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Tiffany Fabianac Modified code from:\n",
    "Reading GMAIL using Python\n",
    "    - https://github.com/abhishekchhibber/Gmail-Api-through-Python\n",
    "\t- Abhishek Chhibber\n",
    "'''\n",
    "\n",
    "'''\n",
    "This script does the following:\n",
    "- Go to Gmal inbox\n",
    "- Find and read all the Google Alert messages\n",
    "- Extract details (Date, Snippet) and export them to a .csv file / DB\n",
    "'''\n",
    "\n",
    "'''\n",
    "Before running this script, the user should get the authentication by following \n",
    "the link: https://developers.google.com/gmail/api/quickstart/python\n",
    "Also, client_secret.json should be saved in the same directory as this file\n",
    "'''\n",
    "\n",
    "# Importing required libraries\n",
    "from apiclient import discovery\n",
    "from apiclient import errors\n",
    "from httplib2 import Http\n",
    "from oauth2client import file, client, tools\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import dateutil.parser as parser\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import csv\n",
    "import json\n",
    "import io\n",
    "\n",
    "# Creating a storage.JSON file with authentication details\n",
    "SCOPES = 'https://www.googleapis.com/auth/gmail.modify'\n",
    "store = file.Storage('storage.json')\n",
    "creds = store.get()\n",
    "if not creds or creds.invalid:\n",
    "    flow = client.flow_from_clientsecrets('client_secret.json', SCOPES)\n",
    "    creds = tools.run_flow(flow, store)\n",
    "GMAIL = discovery.build('gmail', 'v1', http=creds.authorize(Http()))\n",
    "\n",
    "user_id = 'me'\n",
    "label_id_one = 'INBOX'\n",
    "\n",
    "# Getting Google Alert messages from Inbox\n",
    "alert_msgs = GMAIL.users().messages().list(userId='me', labelIds=[label_id_one], q='from:googlealerts-noreply@google.com').execute()\n",
    "\n",
    "# We get a dictonary. Now reading values for the key 'messages'\n",
    "mssg_list = alert_msgs['messages']\n",
    "\n",
    "final_list = []\n",
    "\n",
    "for mssg in mssg_list:\n",
    "    temp_dict = {}\n",
    "    m_id = mssg['id']  # get id of individual message\n",
    "    message = GMAIL.users().messages().get(userId=user_id, id=m_id).execute()  # fetch the message using API\n",
    "    payld = message['payload'] \n",
    "    headr = payld['headers'] \n",
    "\n",
    "    for two in headr:  \n",
    "        if two['name'] == 'Date':\n",
    "            msg_date = two['value']\n",
    "            date_parse = (parser.parse(msg_date))\n",
    "            m_date = (date_parse.date())\n",
    "            temp_dict['Date'] = str(m_date)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    temp_dict['Snippet'] = message['snippet']\n",
    "\n",
    "\n",
    "    final_list=json.dumps(temp_dict)  \n",
    "    re.sub(r'\\u22c5', '', final_list)\n",
    "    print final_list\n",
    "\n",
    "# exporting .csv\n",
    "with open(\"API_out.csv\", \"a\") as f:\n",
    "    header=[\"Date\", \"Snippet\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=header, delimiter=',')\n",
    "    for x in final_list:\n",
    "        writer.writerow(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date\tTicker\tSnippit\tW/L?\t%Change\n",
    "2017-08-22\tABEO\t Abeona Therapeutics - String Of Pearls Strategy With Numerous Catalysts And A Lot Of Upside\t\t\n",
    "2017-08-22\tALKS\tFDA Approves IRWD's Gout Drug, It's A Thumbs Up For ALXN In EU, CRME Crushed\t\t\n",
    "2017-08-22\tALPMY\tFDA Approves IRWD's Gout Drug, It's A Thumbs Up For ALXN In EU, CRME Crushed\t\t\n",
    "2017-08-22\tALXN\tFDA Approves IRWD's Gout Drug, It's A Thumbs Up For ALXN In EU, CRME Crushed\t\t\n",
    "2017-08-22\tARRY\t Array Biopharma (ARRY) Reaches $8.58 After 7.00% Down Move; Per Se Technologies (PSTI\t\t\n",
    "2017-08-22\tAXON\tFDA Approves IRWD's Gout Drug, It's A Thumbs Up For ALXN In EU, CRME Crushed\t\t\n",
    "2017-08-22\tBLRX\tBioLineRx Moves a Regulatory Submission for BL-8040 Trial\t\t\n",
    "2017-08-22\tBLRX\tBioLineRx Moves a Regulatory Submission for BL-8040 Trial\t\t\n",
    "2017-11-19\tBLRX\t BioLineRx Ltd. (BLRX) Receives Average Recommendation of “Buy” from Analysts The Lincolnian Online Its clinical therapeutic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 The stock symbols had to be manually extracted from the Snippets (Sad day) \n",
    "\n",
    "1.4 Find out if the stocks associate with these Google Alerts had an signigifant (10%) price increase within 5 days (including weekends) from the alert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Collect Historical Stock Data\n",
    "Tiffany Fabianac Modified code from:\n",
    "    - http://pandas-datareader.readthedocs.io/en/latest/remote_data.html\n",
    "'''\n",
    "\n",
    "from pandas_datareader import data\n",
    "import pandas as pd\n",
    "import csv\n",
    "import string\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "\n",
    "def stockData(startDate, endDate, ticker):\n",
    "    # Define which online source one should use\n",
    "    data_source = 'google'\n",
    "\n",
    "    # User pandas_reader.data.DataReader to load the desired data.\n",
    "    panel_data = data.DataReader(ticker, data_source, startDate, endDate)\n",
    "\n",
    "    close = panel_data.ix['Close']\n",
    "    volume = panel_data.ix['Volume']\n",
    "    op = panel_data.ix['Open']\n",
    "    high = panel_data.ix['High']\n",
    "    low = panel_data.ix['Low']\n",
    "\n",
    "    # Getting all weekdays between 01/01/2017 and 12/31/2017\n",
    "    all_weekdays = pd.date_range(start=startDate, end=endDate, freq='B')\n",
    "\n",
    "    # Align new set of dates\n",
    "    close = close.reindex(all_weekdays)\n",
    "    volume = volume.reindex(all_weekdays)\n",
    "    op = op.reindex(all_weekdays)\n",
    "    high = high.reindex(all_weekdays)\n",
    "    low = low.reindex(all_weekdays)\n",
    "\n",
    "    result = pd.concat([close, volume, op, high, low], axis=1, join='inner')\n",
    "    result.columns = ['close', 'volume', 'open', 'high', 'low']\n",
    "    return result\n",
    "\n",
    "\n",
    "def findHigh(startDate, ticker):\n",
    "    # Get date and five days after\n",
    "    temp_date = datetime.datetime.strptime(startDate, \"%Y-%m-%d\")\n",
    "    endDate = temp_date + BDay(5)\n",
    "\n",
    "    result = stockData(startDate, endDate, ticker)\n",
    "    tempHigh = result.nlargest(1, 'high')\n",
    "    high = tempHigh.iloc[0]['high']\n",
    "    return high\n",
    "\n",
    "\n",
    "def openPrice(startDate, ticker):\n",
    "    temp_date = datetime.datetime.strptime(startDate, \"%Y-%m-%d\")\n",
    "    endDate = temp_date + BDay(1)\n",
    "\n",
    "    result = stockData(startDate, endDate, ticker)\n",
    "    open = result.iloc[0]['open']\n",
    "    return open\n",
    "\n",
    "\n",
    "with open('google_alert_data.csv', 'rb') as csvfile:\n",
    "    with open('labeledTrainData.csv', 'wb') as f:\n",
    "        datareader = csv.DictReader(csvfile)\n",
    "        writer = csv.DictWriter(f, fieldnames=datareader.fieldnames, extrasaction='ignore', delimiter=',',\n",
    "                                skipinitialspace=True)\n",
    "        writer.writeheader()\n",
    "        for line in datareader:\n",
    "            if (line['Ticker'] == ''):\n",
    "                pass\n",
    "            elif (line['Snippit'] == ''):\n",
    "                pass\n",
    "            else:\n",
    "                ticker = [line['Ticker']]\n",
    "                date = line['Date']\n",
    "                high = findHigh(date, ticker)\n",
    "                startPrice = openPrice(date, ticker)\n",
    "                prctIncrease = round(((high - startPrice) / startPrice) * 100, 2)\n",
    "                if (high > startPrice * 1.1):\n",
    "                    line['W/L?'] = 'W'\n",
    "                    line['%Change'] = prctIncrease\n",
    "                    print ticker, prctIncrease, high, startPrice, date\n",
    "                else:\n",
    "                    line['W/L?'] = 'L'\n",
    "                    line['%Change'] = prctIncrease\n",
    "                line['Snippit'] = line['Snippit'].replace(',', '')\n",
    "                writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Date\tTicker\tSnippit\tW/L?\t%Change\n",
    "2017-08-22\tABEO\t Abeona Therapeutics - String Of Pearls Strategy With Numerous Catalysts And A Lot Of Upside\tW\t46.1\n",
    "2017-08-22\tALKS\tFDA Approves IRWD's Gout Drug It's A Thumbs Up For ALXN In EU CRME Crushed\tL\t0.39\n",
    "2017-08-22\tALPMY\tFDA Approves IRWD's Gout Drug It's A Thumbs Up For ALXN In EU CRME Crushed\tL\t1.6\n",
    "2017-08-22\tALXN\tFDA Approves IRWD's Gout Drug It's A Thumbs Up For ALXN In EU CRME Crushed\tL\t5.28\n",
    "2017-08-22\tARRY\t Array Biopharma (ARRY) Reaches $8.58 After 7.00% Down Move; Per Se Technologies (PSTI\tW\t16.34\n",
    "2017-08-22\tAXON\tFDA Approves IRWD's Gout Drug It's A Thumbs Up For ALXN In EU CRME Crushed\tL\t1.24\n",
    "2017-08-22\tBLRX\tBioLineRx Moves a Regulatory Submission for BL-8040 Trial\tL\t6.42\n",
    "2017-08-22\tBLRX\tBioLineRx Moves a Regulatory Submission for BL-8040 Trial\tL\t6.42\n",
    "2017-11-19\tBLRX\t BioLineRx Ltd. (BLRX) Receives Average Recommendation of “Buy” from Analysts The Lincolnian Online Its clinical therapeutic\tL\t6.48\n",
    "2017-08-22\tBPMC\tFDA Approves IRWD's Gout Drug It's A Thumbs Up For ALXN In EU CRME Crushed\tW\t24.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2) Analyze Data using random tree analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Use KaggleWord to produce random forest analysis\n",
    "Tiffany Fabianac Modified code from:\n",
    "    - https://youtu.be/AJVP96tAWxw\n",
    "    - Siraj Raval\n",
    "'''\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from KaggleWord2VecUtility import KaggleWord2VecUtility\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train = pd.read_csv(os.path.join(os.path.dirname(__file__), 'labeledTrainData.csv'), header=0, delimiter=\",\", quoting=3)\n",
    "    test = pd.read_csv(os.path.join(os.path.dirname(__file__), 'testData.csv'), header=0, delimiter=\",\", quoting=3)\n",
    "\n",
    "    print 'The first Headline is:'\n",
    "    print train['Snippit'][0]\n",
    "    raw_input(\"Press Enter to continue...\")\n",
    "\n",
    "    #Download text data sets\n",
    "    nltk.download()\n",
    "    clean_train_reviews = []\n",
    "    #Cleaning and parsing the training set\n",
    "    for i in xrange(0, len(train['Snippit'])):\n",
    "        clean_train_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(train['Snippit'][i], True)))\n",
    "\n",
    "    #Creating the bag of words\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=None, preprocessor=None, stop_words=None, max_features=5000)\n",
    "    train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "    train_data_features = train_data_features.toarray()\n",
    "\n",
    "    #Training Random forest\n",
    "    forest = RandomForestClassifier(n_estimators=100)\n",
    "    forest = forest.fit(train_data_features, train['W/L?'])\n",
    "    clean_test_reviews=[]\n",
    "\n",
    "    #Cleaning and parsing\n",
    "    for i in xrange(0,len(test['Snippit'])):\n",
    "        clean_test_reviews.append(\" \".join(KaggleWord2VecUtility.review_to_wordlist(test['Snippit'][i], True)))\n",
    "    test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "    test_data_features = test_data_features.toarray()\n",
    "\n",
    "    #Predicting test labels\n",
    "    result = forest.predict(test_data_features)\n",
    "    output = pd.DataFrame(data={\"Accuracy\":\"\", \"Sentiment\":result, \"Ticker\":test[\"Ticker\"], \"Date\":test[\"Date\"]})\n",
    "    output.to_csv(os.path.join(os.path.dirname(__file__), 'randomForestResults.csv'), index=False, quoting=3)\n",
    "    print \"Wrote results to randomForestResults.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\tDate\tSentiment\tTicker\n",
    "           2017-12-03\tL\tABBV\n",
    "           2017-12-01\tL\tABBV\n",
    "           2017-11-30\tL\tACAD\n",
    "           2017-12-02\tL\tALNY\n",
    "           2017-12-03\tL\tARGX\n",
    "           2017-12-02\tL\tBABA\n",
    "           2017-12-01\tL\tBAYN\n",
    "           2017-12-03\tL\tBPMX\n",
    "           2017-12-02\tL\tCISN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3) Determine Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Validate random forest analysis\n",
    "Tiffany Fabianac Modified code from:\n",
    "    - http://pandas-datareader.readthedocs.io/en/latest/remote_data.html\n",
    "'''\n",
    "\n",
    "from pandas_datareader import data\n",
    "import pandas as pd\n",
    "import csv\n",
    "import string\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "\n",
    "def stockData(startDate, endDate, ticker):\n",
    "    # Define which online source one should use\n",
    "    data_source = 'google'\n",
    "\n",
    "    # User pandas_reader.data.DataReader to load the desired data.\n",
    "    panel_data = data.DataReader(ticker, data_source, startDate, endDate)\n",
    "\n",
    "    close = panel_data.ix['Close']\n",
    "    volume = panel_data.ix['Volume']\n",
    "    op = panel_data.ix['Open']\n",
    "    high = panel_data.ix['High']\n",
    "    low = panel_data.ix['Low']\n",
    "\n",
    "    # Getting all weekdays between 01/01/2017 and 12/31/2017\n",
    "    all_weekdays = pd.date_range(start=startDate, end=endDate, freq='B')\n",
    "\n",
    "    # Align new set of dates\n",
    "    close = close.reindex(all_weekdays)\n",
    "    volume = volume.reindex(all_weekdays)\n",
    "    op = op.reindex(all_weekdays)\n",
    "    high = high.reindex(all_weekdays)\n",
    "    low = low.reindex(all_weekdays)\n",
    "\n",
    "    result = pd.concat([close, volume, op, high, low], axis=1, join='inner')\n",
    "    result.columns = ['close', 'volume', 'open', 'high', 'low']\n",
    "    return result\n",
    "\n",
    "\n",
    "def findHigh(startDate, ticker):\n",
    "    # Get date and five days after\n",
    "    endDate = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    result = stockData(startDate, endDate, ticker)\n",
    "    if (result.iloc[0]['high'] != result.iloc[0]['high']):\n",
    "        return 0\n",
    "    else:\n",
    "        tempHigh = result.nlargest(1, 'high')\n",
    "        high = tempHigh.iloc[0]['high']\n",
    "        return high\n",
    "\n",
    "\n",
    "def openPrice(endDate, ticker):\n",
    "    temp_date = datetime.datetime.strptime(endDate, \"%Y-%m-%d\")\n",
    "    startDate = temp_date - BDay(1)\n",
    "\n",
    "    result = stockData(startDate, endDate, ticker)\n",
    "    if (result.iloc[0]['high'] != result.iloc[0]['high']):\n",
    "        return 1\n",
    "    else:\n",
    "        open = result.iloc[0]['open']\n",
    "        return open\n",
    "\n",
    "\n",
    "with open('randomForestResults.csv', 'rb') as csvfile:\n",
    "    with open('resultsData.csv', 'wb') as f:\n",
    "        datareader = csv.DictReader(csvfile)\n",
    "        writer = csv.DictWriter(f, fieldnames=datareader.fieldnames, extrasaction='ignore', delimiter=',',\n",
    "                                skipinitialspace=True)\n",
    "        writer.writeheader()\n",
    "        for line in datareader:\n",
    "            if (line['Ticker'] == '' or line['Sentiment'] == ''):\n",
    "                pass\n",
    "            else:\n",
    "                ticker = [line['Ticker']]\n",
    "                date = line['Date']\n",
    "                high = findHigh(date, ticker)\n",
    "                startPrice = openPrice(date, ticker)\n",
    "                prctIncrease = round(((high - startPrice) / startPrice) * 100, 2)\n",
    "                if (high > startPrice * 1.1 and line['Sentiment'] == 'W'):\n",
    "                    line['Accuracy'] = 'W'\n",
    "                    print ticker, prctIncrease, high, startPrice, date\n",
    "                else:\n",
    "                    line['Accuracy'] = 'L'\n",
    "                writer.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Accuracy,Date,Sentiment,Ticker\n",
    "L,2017-12-03,L,ABBV\n",
    "L,2017-12-01,L,ABBV\n",
    "L,2017-11-30,L,ACAD\n",
    "L,2017-12-02,L,ALNY\n",
    "L,2017-12-03,L,ARGX\n",
    "L,2017-12-02,L,BABA\n",
    "L,2017-12-01,L,BAYN\n",
    "L,2017-12-03,L,BPMX\n",
    "L,2017-12-02,L,CISN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
